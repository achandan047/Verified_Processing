lr: 0.001
batch_size: 32
eval_batch_size: 64
gradient_accumulation_steps: 1
num_epochs: 5
early_stopping: 3
device: cuda:0
stamp: frame_adafactor_t5base
base_path: models/
# trained_learner: arg_adafactor_base-t5model.h5
train: True
frame_predfile: pipeline/frame_predictions.npy